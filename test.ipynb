{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPaddin (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 4, 4, 256)         295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 4097      \n",
      "=================================================================\n",
      "Total params: 393,729\n",
      "Trainable params: 392,833\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 6272)              633472    \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 14, 14, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 28, 28, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 28, 28, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 28, 28, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 28, 28, 1)         577       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 856,193\n",
      "Trainable params: 855,809\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "0 [D loss: 1.070559, acc.: 32.81%] [G loss: 0.922300]\n",
      "1 [D loss: 0.659614, acc.: 62.50%] [G loss: 0.953302]\n",
      "2 [D loss: 0.678455, acc.: 60.94%] [G loss: 1.088939]\n",
      "3 [D loss: 0.765931, acc.: 56.25%] [G loss: 0.994822]\n",
      "4 [D loss: 0.467618, acc.: 79.69%] [G loss: 0.906006]\n",
      "5 [D loss: 0.457711, acc.: 79.69%] [G loss: 1.031862]\n",
      "6 [D loss: 0.618802, acc.: 70.31%] [G loss: 1.008267]\n",
      "7 [D loss: 0.691280, acc.: 67.19%] [G loss: 1.611488]\n",
      "8 [D loss: 0.889587, acc.: 43.75%] [G loss: 1.452237]\n",
      "9 [D loss: 0.596601, acc.: 71.88%] [G loss: 1.663377]\n",
      "10 [D loss: 0.574021, acc.: 76.56%] [G loss: 1.401014]\n",
      "11 [D loss: 0.518842, acc.: 81.25%] [G loss: 1.289010]\n",
      "12 [D loss: 0.428169, acc.: 81.25%] [G loss: 1.487468]\n",
      "13 [D loss: 0.830684, acc.: 57.81%] [G loss: 1.846374]\n",
      "14 [D loss: 1.065661, acc.: 45.31%] [G loss: 1.412633]\n",
      "15 [D loss: 0.867789, acc.: 59.38%] [G loss: 1.186197]\n",
      "16 [D loss: 0.838139, acc.: 56.25%] [G loss: 1.295355]\n",
      "17 [D loss: 0.541974, acc.: 70.31%] [G loss: 1.482164]\n",
      "18 [D loss: 0.927383, acc.: 53.12%] [G loss: 1.648488]\n",
      "19 [D loss: 0.857478, acc.: 48.44%] [G loss: 1.939729]\n",
      "20 [D loss: 0.978537, acc.: 46.88%] [G loss: 1.205364]\n",
      "21 [D loss: 1.113771, acc.: 37.50%] [G loss: 0.761854]\n",
      "22 [D loss: 0.850937, acc.: 57.81%] [G loss: 0.854644]\n",
      "23 [D loss: 0.947748, acc.: 45.31%] [G loss: 0.714677]\n",
      "24 [D loss: 1.116888, acc.: 42.19%] [G loss: 1.299061]\n",
      "25 [D loss: 1.009710, acc.: 37.50%] [G loss: 1.307296]\n",
      "26 [D loss: 0.958282, acc.: 46.88%] [G loss: 1.271899]\n",
      "27 [D loss: 0.569359, acc.: 68.75%] [G loss: 1.139063]\n",
      "28 [D loss: 0.529732, acc.: 76.56%] [G loss: 0.493378]\n",
      "29 [D loss: 0.429261, acc.: 78.12%] [G loss: 0.687411]\n",
      "30 [D loss: 0.524524, acc.: 79.69%] [G loss: 1.131303]\n",
      "31 [D loss: 0.722565, acc.: 64.06%] [G loss: 1.539378]\n",
      "32 [D loss: 1.113806, acc.: 35.94%] [G loss: 1.051540]\n",
      "33 [D loss: 1.188288, acc.: 25.00%] [G loss: 1.103275]\n",
      "34 [D loss: 0.780235, acc.: 50.00%] [G loss: 0.968070]\n",
      "35 [D loss: 0.534367, acc.: 71.88%] [G loss: 1.008357]\n",
      "36 [D loss: 0.772225, acc.: 56.25%] [G loss: 0.734088]\n",
      "37 [D loss: 1.142923, acc.: 26.56%] [G loss: 1.189008]\n",
      "38 [D loss: 1.098547, acc.: 32.81%] [G loss: 1.646167]\n",
      "39 [D loss: 0.928524, acc.: 54.69%] [G loss: 1.791087]\n",
      "40 [D loss: 0.876955, acc.: 43.75%] [G loss: 1.067713]\n",
      "41 [D loss: 0.689399, acc.: 62.50%] [G loss: 0.987980]\n",
      "42 [D loss: 0.681480, acc.: 60.94%] [G loss: 1.350446]\n",
      "43 [D loss: 0.903501, acc.: 45.31%] [G loss: 1.693889]\n",
      "44 [D loss: 1.091215, acc.: 34.38%] [G loss: 1.192637]\n",
      "45 [D loss: 0.928992, acc.: 43.75%] [G loss: 1.165852]\n",
      "46 [D loss: 0.636067, acc.: 65.62%] [G loss: 0.859318]\n",
      "47 [D loss: 0.745626, acc.: 60.94%] [G loss: 0.959962]\n",
      "48 [D loss: 0.607317, acc.: 67.19%] [G loss: 0.835482]\n",
      "49 [D loss: 0.904244, acc.: 46.88%] [G loss: 1.150201]\n",
      "50 [D loss: 1.159529, acc.: 31.25%] [G loss: 1.205575]\n",
      "51 [D loss: 0.876205, acc.: 60.94%] [G loss: 1.055937]\n",
      "52 [D loss: 0.749039, acc.: 57.81%] [G loss: 1.066603]\n",
      "53 [D loss: 0.642746, acc.: 64.06%] [G loss: 0.913221]\n",
      "54 [D loss: 0.566876, acc.: 65.62%] [G loss: 1.225805]\n",
      "55 [D loss: 1.015274, acc.: 46.88%] [G loss: 1.620992]\n",
      "56 [D loss: 0.951066, acc.: 46.88%] [G loss: 1.194982]\n",
      "57 [D loss: 0.811322, acc.: 43.75%] [G loss: 1.298218]\n",
      "58 [D loss: 0.812933, acc.: 48.44%] [G loss: 1.103208]\n",
      "59 [D loss: 0.626370, acc.: 64.06%] [G loss: 0.929949]\n",
      "60 [D loss: 0.794729, acc.: 57.81%] [G loss: 1.084201]\n",
      "61 [D loss: 0.879761, acc.: 48.44%] [G loss: 1.599480]\n",
      "62 [D loss: 0.835612, acc.: 48.44%] [G loss: 1.578948]\n",
      "63 [D loss: 0.820036, acc.: 43.75%] [G loss: 1.013088]\n",
      "64 [D loss: 0.667237, acc.: 59.38%] [G loss: 1.105835]\n",
      "65 [D loss: 0.909618, acc.: 53.12%] [G loss: 0.911922]\n",
      "66 [D loss: 0.746887, acc.: 57.81%] [G loss: 1.199613]\n",
      "67 [D loss: 0.914269, acc.: 37.50%] [G loss: 1.472218]\n",
      "68 [D loss: 1.000786, acc.: 37.50%] [G loss: 1.610791]\n",
      "69 [D loss: 0.862968, acc.: 56.25%] [G loss: 1.199329]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 [D loss: 0.762781, acc.: 59.38%] [G loss: 0.906338]\n",
      "71 [D loss: 0.662751, acc.: 60.94%] [G loss: 0.872865]\n",
      "72 [D loss: 0.888864, acc.: 46.88%] [G loss: 1.079451]\n",
      "73 [D loss: 0.823328, acc.: 46.88%] [G loss: 1.297596]\n",
      "74 [D loss: 1.055216, acc.: 42.19%] [G loss: 1.263545]\n",
      "75 [D loss: 1.096108, acc.: 35.94%] [G loss: 1.246214]\n",
      "76 [D loss: 0.782548, acc.: 60.94%] [G loss: 1.361449]\n",
      "77 [D loss: 0.984988, acc.: 39.06%] [G loss: 1.064546]\n",
      "78 [D loss: 0.897339, acc.: 48.44%] [G loss: 1.256221]\n",
      "79 [D loss: 0.979739, acc.: 42.19%] [G loss: 1.131291]\n",
      "80 [D loss: 0.851675, acc.: 50.00%] [G loss: 1.143041]\n",
      "81 [D loss: 0.887380, acc.: 51.56%] [G loss: 1.206262]\n",
      "82 [D loss: 0.899133, acc.: 48.44%] [G loss: 1.073696]\n",
      "83 [D loss: 0.780976, acc.: 53.12%] [G loss: 1.160697]\n",
      "84 [D loss: 0.887362, acc.: 37.50%] [G loss: 1.176769]\n",
      "85 [D loss: 0.993293, acc.: 43.75%] [G loss: 1.185020]\n",
      "86 [D loss: 1.119791, acc.: 34.38%] [G loss: 0.928324]\n",
      "87 [D loss: 0.983354, acc.: 42.19%] [G loss: 1.059914]\n",
      "88 [D loss: 0.972534, acc.: 46.88%] [G loss: 1.239463]\n",
      "89 [D loss: 0.896906, acc.: 45.31%] [G loss: 0.965251]\n",
      "90 [D loss: 0.844614, acc.: 54.69%] [G loss: 1.016861]\n",
      "91 [D loss: 0.797738, acc.: 45.31%] [G loss: 1.206733]\n",
      "92 [D loss: 0.826885, acc.: 53.12%] [G loss: 1.354383]\n",
      "93 [D loss: 0.835421, acc.: 43.75%] [G loss: 1.336673]\n",
      "94 [D loss: 0.889250, acc.: 53.12%] [G loss: 1.001991]\n",
      "95 [D loss: 1.126495, acc.: 37.50%] [G loss: 0.926043]\n",
      "96 [D loss: 0.851996, acc.: 48.44%] [G loss: 1.201741]\n",
      "97 [D loss: 0.913297, acc.: 53.12%] [G loss: 1.323031]\n",
      "98 [D loss: 1.034153, acc.: 37.50%] [G loss: 1.262534]\n",
      "99 [D loss: 0.982662, acc.: 46.88%] [G loss: 1.397700]\n",
      "100 [D loss: 0.911986, acc.: 40.62%] [G loss: 0.955887]\n",
      "101 [D loss: 0.894026, acc.: 42.19%] [G loss: 1.021453]\n",
      "102 [D loss: 0.991641, acc.: 37.50%] [G loss: 1.036879]\n",
      "103 [D loss: 0.857762, acc.: 51.56%] [G loss: 1.305186]\n",
      "104 [D loss: 1.102386, acc.: 32.81%] [G loss: 0.869022]\n",
      "105 [D loss: 0.860610, acc.: 50.00%] [G loss: 1.053530]\n",
      "106 [D loss: 0.789358, acc.: 57.81%] [G loss: 0.961078]\n",
      "107 [D loss: 1.065231, acc.: 40.62%] [G loss: 1.057929]\n",
      "108 [D loss: 0.811064, acc.: 54.69%] [G loss: 0.951393]\n",
      "109 [D loss: 0.782665, acc.: 54.69%] [G loss: 0.969435]\n",
      "110 [D loss: 0.858165, acc.: 40.62%] [G loss: 1.164518]\n",
      "111 [D loss: 0.922037, acc.: 39.06%] [G loss: 1.110727]\n",
      "112 [D loss: 1.038168, acc.: 34.38%] [G loss: 1.078923]\n",
      "113 [D loss: 0.783409, acc.: 45.31%] [G loss: 1.107327]\n",
      "114 [D loss: 0.884999, acc.: 39.06%] [G loss: 0.985733]\n",
      "115 [D loss: 0.959606, acc.: 39.06%] [G loss: 0.988108]\n",
      "116 [D loss: 0.777443, acc.: 50.00%] [G loss: 1.249763]\n",
      "117 [D loss: 0.917312, acc.: 50.00%] [G loss: 0.808508]\n",
      "118 [D loss: 0.844077, acc.: 46.88%] [G loss: 1.081610]\n",
      "119 [D loss: 0.770929, acc.: 50.00%] [G loss: 1.000732]\n",
      "120 [D loss: 0.757116, acc.: 54.69%] [G loss: 1.219766]\n",
      "121 [D loss: 0.844616, acc.: 43.75%] [G loss: 0.873625]\n",
      "122 [D loss: 0.802274, acc.: 51.56%] [G loss: 1.092615]\n",
      "123 [D loss: 0.839863, acc.: 42.19%] [G loss: 1.204432]\n",
      "124 [D loss: 1.069559, acc.: 29.69%] [G loss: 1.029257]\n",
      "125 [D loss: 0.847954, acc.: 51.56%] [G loss: 1.109435]\n",
      "126 [D loss: 0.952329, acc.: 45.31%] [G loss: 0.989715]\n",
      "127 [D loss: 0.806560, acc.: 48.44%] [G loss: 0.978676]\n",
      "128 [D loss: 0.824971, acc.: 51.56%] [G loss: 1.205261]\n",
      "129 [D loss: 0.897543, acc.: 37.50%] [G loss: 1.295024]\n",
      "130 [D loss: 0.855700, acc.: 39.06%] [G loss: 0.979968]\n",
      "131 [D loss: 0.872889, acc.: 42.19%] [G loss: 1.179806]\n",
      "132 [D loss: 0.758380, acc.: 56.25%] [G loss: 1.093827]\n",
      "133 [D loss: 0.834866, acc.: 46.88%] [G loss: 0.921960]\n",
      "134 [D loss: 0.933012, acc.: 40.62%] [G loss: 1.288040]\n",
      "135 [D loss: 0.934114, acc.: 51.56%] [G loss: 1.390162]\n",
      "136 [D loss: 0.935134, acc.: 42.19%] [G loss: 1.088717]\n",
      "137 [D loss: 0.831477, acc.: 46.88%] [G loss: 0.935501]\n",
      "138 [D loss: 0.813480, acc.: 42.19%] [G loss: 1.169428]\n",
      "139 [D loss: 0.791615, acc.: 51.56%] [G loss: 1.021388]\n",
      "140 [D loss: 0.782065, acc.: 48.44%] [G loss: 1.018890]\n",
      "141 [D loss: 0.803103, acc.: 60.94%] [G loss: 1.167987]\n",
      "142 [D loss: 0.856134, acc.: 42.19%] [G loss: 1.272738]\n",
      "143 [D loss: 0.878556, acc.: 45.31%] [G loss: 0.924572]\n",
      "144 [D loss: 0.889693, acc.: 46.88%] [G loss: 1.049879]\n",
      "145 [D loss: 0.761855, acc.: 56.25%] [G loss: 1.207269]\n",
      "146 [D loss: 0.829855, acc.: 51.56%] [G loss: 1.250011]\n",
      "147 [D loss: 1.007944, acc.: 40.62%] [G loss: 1.155507]\n",
      "148 [D loss: 0.839434, acc.: 45.31%] [G loss: 0.965762]\n",
      "149 [D loss: 0.912077, acc.: 45.31%] [G loss: 0.947263]\n",
      "150 [D loss: 0.953990, acc.: 37.50%] [G loss: 0.978178]\n",
      "151 [D loss: 0.809111, acc.: 56.25%] [G loss: 1.163202]\n",
      "152 [D loss: 0.866236, acc.: 45.31%] [G loss: 1.357145]\n",
      "153 [D loss: 0.868400, acc.: 50.00%] [G loss: 1.176511]\n",
      "154 [D loss: 1.004256, acc.: 35.94%] [G loss: 0.914481]\n",
      "155 [D loss: 0.884845, acc.: 48.44%] [G loss: 1.137219]\n",
      "156 [D loss: 0.951961, acc.: 43.75%] [G loss: 1.274478]\n",
      "157 [D loss: 0.780478, acc.: 56.25%] [G loss: 0.952676]\n",
      "158 [D loss: 0.801044, acc.: 46.88%] [G loss: 1.173209]\n",
      "159 [D loss: 0.880959, acc.: 45.31%] [G loss: 1.327347]\n",
      "160 [D loss: 0.853955, acc.: 43.75%] [G loss: 0.878342]\n",
      "161 [D loss: 0.749178, acc.: 59.38%] [G loss: 0.735055]\n",
      "162 [D loss: 0.820794, acc.: 48.44%] [G loss: 1.151164]\n",
      "163 [D loss: 0.938726, acc.: 43.75%] [G loss: 0.977912]\n",
      "164 [D loss: 0.773571, acc.: 53.12%] [G loss: 0.983480]\n",
      "165 [D loss: 0.953312, acc.: 32.81%] [G loss: 0.886487]\n",
      "166 [D loss: 0.795433, acc.: 50.00%] [G loss: 0.914302]\n",
      "167 [D loss: 0.937766, acc.: 40.62%] [G loss: 1.047999]\n",
      "168 [D loss: 0.791787, acc.: 50.00%] [G loss: 1.137648]\n",
      "169 [D loss: 0.985034, acc.: 37.50%] [G loss: 0.950022]\n",
      "170 [D loss: 0.847618, acc.: 51.56%] [G loss: 1.228685]\n",
      "171 [D loss: 0.787408, acc.: 51.56%] [G loss: 1.050635]\n",
      "172 [D loss: 0.909586, acc.: 40.62%] [G loss: 1.019981]\n",
      "173 [D loss: 0.769325, acc.: 48.44%] [G loss: 0.982982]\n",
      "174 [D loss: 0.819998, acc.: 57.81%] [G loss: 1.044469]\n",
      "175 [D loss: 0.836881, acc.: 51.56%] [G loss: 0.935720]\n",
      "176 [D loss: 0.870649, acc.: 46.88%] [G loss: 1.224787]\n",
      "177 [D loss: 0.894746, acc.: 42.19%] [G loss: 1.042848]\n",
      "178 [D loss: 0.860117, acc.: 42.19%] [G loss: 1.064541]\n",
      "179 [D loss: 0.909292, acc.: 39.06%] [G loss: 0.926335]\n",
      "180 [D loss: 0.874000, acc.: 46.88%] [G loss: 1.132654]\n",
      "181 [D loss: 0.694756, acc.: 59.38%] [G loss: 1.169854]\n",
      "182 [D loss: 0.973016, acc.: 39.06%] [G loss: 0.812764]\n",
      "183 [D loss: 0.831083, acc.: 53.12%] [G loss: 1.075988]\n",
      "184 [D loss: 0.766288, acc.: 51.56%] [G loss: 0.936951]\n",
      "185 [D loss: 0.797648, acc.: 46.88%] [G loss: 0.803463]\n",
      "186 [D loss: 0.760491, acc.: 48.44%] [G loss: 0.878712]\n",
      "187 [D loss: 0.765988, acc.: 50.00%] [G loss: 1.128382]\n",
      "188 [D loss: 0.743051, acc.: 54.69%] [G loss: 0.905588]\n",
      "189 [D loss: 0.884061, acc.: 43.75%] [G loss: 1.013669]\n",
      "190 [D loss: 0.897785, acc.: 45.31%] [G loss: 1.127051]\n",
      "191 [D loss: 0.739052, acc.: 56.25%] [G loss: 0.997751]\n",
      "192 [D loss: 0.863362, acc.: 43.75%] [G loss: 0.882301]\n",
      "193 [D loss: 0.817534, acc.: 48.44%] [G loss: 0.953167]\n",
      "194 [D loss: 0.790916, acc.: 46.88%] [G loss: 0.997480]\n",
      "195 [D loss: 0.760499, acc.: 48.44%] [G loss: 0.980359]\n",
      "196 [D loss: 0.912115, acc.: 37.50%] [G loss: 0.847217]\n",
      "197 [D loss: 0.787779, acc.: 50.00%] [G loss: 1.072017]\n",
      "198 [D loss: 0.823143, acc.: 40.62%] [G loss: 0.919814]\n",
      "199 [D loss: 0.822123, acc.: 51.56%] [G loss: 0.912268]\n",
      "200 [D loss: 0.846547, acc.: 46.88%] [G loss: 0.999341]\n",
      "201 [D loss: 0.869349, acc.: 46.88%] [G loss: 1.130365]\n",
      "202 [D loss: 0.802226, acc.: 46.88%] [G loss: 1.073205]\n",
      "203 [D loss: 1.007415, acc.: 28.12%] [G loss: 1.087909]\n",
      "204 [D loss: 0.895429, acc.: 45.31%] [G loss: 1.195870]\n",
      "205 [D loss: 0.853378, acc.: 45.31%] [G loss: 0.920716]\n",
      "206 [D loss: 0.837383, acc.: 45.31%] [G loss: 1.090225]\n",
      "207 [D loss: 0.875881, acc.: 46.88%] [G loss: 0.963831]\n",
      "208 [D loss: 0.900852, acc.: 39.06%] [G loss: 0.941851]\n",
      "209 [D loss: 0.802644, acc.: 50.00%] [G loss: 1.016337]\n",
      "210 [D loss: 0.781820, acc.: 48.44%] [G loss: 1.078004]\n",
      "211 [D loss: 0.928682, acc.: 42.19%] [G loss: 0.972184]\n",
      "212 [D loss: 0.760975, acc.: 64.06%] [G loss: 1.115150]\n",
      "213 [D loss: 0.817801, acc.: 43.75%] [G loss: 1.060416]\n",
      "214 [D loss: 0.827124, acc.: 40.62%] [G loss: 1.137564]\n",
      "215 [D loss: 0.872576, acc.: 40.62%] [G loss: 0.848481]\n",
      "216 [D loss: 0.857522, acc.: 35.94%] [G loss: 0.830423]\n",
      "217 [D loss: 0.811615, acc.: 48.44%] [G loss: 0.992340]\n",
      "218 [D loss: 0.831496, acc.: 45.31%] [G loss: 1.061957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219 [D loss: 0.853937, acc.: 40.62%] [G loss: 0.961140]\n",
      "220 [D loss: 0.776743, acc.: 48.44%] [G loss: 1.002175]\n",
      "221 [D loss: 0.852411, acc.: 35.94%] [G loss: 0.904047]\n",
      "222 [D loss: 0.793900, acc.: 51.56%] [G loss: 1.063722]\n",
      "223 [D loss: 0.922010, acc.: 45.31%] [G loss: 1.088577]\n",
      "224 [D loss: 0.922135, acc.: 46.88%] [G loss: 0.864376]\n",
      "225 [D loss: 0.760819, acc.: 42.19%] [G loss: 1.101758]\n",
      "226 [D loss: 0.930544, acc.: 42.19%] [G loss: 0.958565]\n",
      "227 [D loss: 0.844191, acc.: 50.00%] [G loss: 1.092203]\n",
      "228 [D loss: 0.858289, acc.: 46.88%] [G loss: 0.988899]\n",
      "229 [D loss: 0.806957, acc.: 51.56%] [G loss: 0.954975]\n",
      "230 [D loss: 0.905625, acc.: 45.31%] [G loss: 0.959502]\n",
      "231 [D loss: 0.861151, acc.: 43.75%] [G loss: 1.057131]\n",
      "232 [D loss: 0.813060, acc.: 46.88%] [G loss: 0.983158]\n",
      "233 [D loss: 0.696446, acc.: 64.06%] [G loss: 1.092681]\n",
      "234 [D loss: 0.760346, acc.: 54.69%] [G loss: 1.074688]\n",
      "235 [D loss: 0.827270, acc.: 42.19%] [G loss: 0.930024]\n",
      "236 [D loss: 0.779669, acc.: 51.56%] [G loss: 1.086814]\n",
      "237 [D loss: 0.723016, acc.: 56.25%] [G loss: 0.817096]\n",
      "238 [D loss: 0.879190, acc.: 42.19%] [G loss: 1.141268]\n",
      "239 [D loss: 0.819706, acc.: 48.44%] [G loss: 0.959942]\n",
      "240 [D loss: 0.841706, acc.: 45.31%] [G loss: 0.897344]\n",
      "241 [D loss: 0.815508, acc.: 50.00%] [G loss: 1.245696]\n",
      "242 [D loss: 0.813188, acc.: 40.62%] [G loss: 1.053355]\n",
      "243 [D loss: 0.770049, acc.: 53.12%] [G loss: 0.839068]\n",
      "244 [D loss: 0.829796, acc.: 51.56%] [G loss: 0.890073]\n",
      "245 [D loss: 0.739032, acc.: 56.25%] [G loss: 0.830541]\n",
      "246 [D loss: 0.935247, acc.: 39.06%] [G loss: 0.964815]\n",
      "247 [D loss: 0.822697, acc.: 50.00%] [G loss: 1.032519]\n",
      "248 [D loss: 0.867061, acc.: 46.88%] [G loss: 0.948893]\n",
      "249 [D loss: 0.694921, acc.: 59.38%] [G loss: 1.044438]\n",
      "250 [D loss: 0.908577, acc.: 34.38%] [G loss: 0.922007]\n",
      "251 [D loss: 1.066527, acc.: 28.12%] [G loss: 1.004916]\n",
      "252 [D loss: 0.887565, acc.: 34.38%] [G loss: 0.899181]\n",
      "253 [D loss: 0.970549, acc.: 37.50%] [G loss: 0.925536]\n",
      "254 [D loss: 0.875028, acc.: 42.19%] [G loss: 1.001466]\n",
      "255 [D loss: 0.808358, acc.: 46.88%] [G loss: 1.090578]\n",
      "256 [D loss: 0.839313, acc.: 43.75%] [G loss: 1.055027]\n",
      "257 [D loss: 0.772753, acc.: 48.44%] [G loss: 1.006068]\n",
      "258 [D loss: 0.884102, acc.: 42.19%] [G loss: 0.865185]\n",
      "259 [D loss: 0.825497, acc.: 51.56%] [G loss: 0.766034]\n",
      "260 [D loss: 0.733887, acc.: 53.12%] [G loss: 1.126965]\n",
      "261 [D loss: 0.802826, acc.: 53.12%] [G loss: 1.237425]\n",
      "262 [D loss: 0.813516, acc.: 50.00%] [G loss: 0.976918]\n",
      "263 [D loss: 0.762059, acc.: 46.88%] [G loss: 0.983315]\n",
      "264 [D loss: 0.880015, acc.: 37.50%] [G loss: 0.793841]\n",
      "265 [D loss: 0.820856, acc.: 46.88%] [G loss: 0.867446]\n",
      "266 [D loss: 0.848776, acc.: 40.62%] [G loss: 1.053816]\n",
      "267 [D loss: 0.799889, acc.: 53.12%] [G loss: 0.908863]\n",
      "268 [D loss: 0.793436, acc.: 51.56%] [G loss: 1.067534]\n",
      "269 [D loss: 0.762931, acc.: 54.69%] [G loss: 0.937276]\n",
      "270 [D loss: 0.670013, acc.: 51.56%] [G loss: 1.028116]\n",
      "271 [D loss: 0.849282, acc.: 35.94%] [G loss: 1.147108]\n",
      "272 [D loss: 0.773544, acc.: 50.00%] [G loss: 0.989941]\n",
      "273 [D loss: 0.857208, acc.: 39.06%] [G loss: 0.862335]\n",
      "274 [D loss: 0.698929, acc.: 59.38%] [G loss: 1.258161]\n",
      "275 [D loss: 0.818200, acc.: 50.00%] [G loss: 1.087641]\n",
      "276 [D loss: 0.837427, acc.: 42.19%] [G loss: 0.939837]\n",
      "277 [D loss: 0.799609, acc.: 54.69%] [G loss: 0.995195]\n",
      "278 [D loss: 0.843308, acc.: 51.56%] [G loss: 0.963391]\n",
      "279 [D loss: 0.854131, acc.: 40.62%] [G loss: 0.921200]\n",
      "280 [D loss: 0.831005, acc.: 40.62%] [G loss: 0.945152]\n",
      "281 [D loss: 0.769432, acc.: 54.69%] [G loss: 0.984984]\n",
      "282 [D loss: 0.857813, acc.: 46.88%] [G loss: 1.139549]\n",
      "283 [D loss: 0.731302, acc.: 53.12%] [G loss: 0.863935]\n",
      "284 [D loss: 0.864748, acc.: 51.56%] [G loss: 0.966325]\n",
      "285 [D loss: 0.858395, acc.: 50.00%] [G loss: 1.049877]\n",
      "286 [D loss: 0.926743, acc.: 39.06%] [G loss: 0.969840]\n",
      "287 [D loss: 0.910827, acc.: 35.94%] [G loss: 0.911300]\n",
      "288 [D loss: 0.793442, acc.: 46.88%] [G loss: 0.850485]\n",
      "289 [D loss: 0.762275, acc.: 45.31%] [G loss: 0.930133]\n",
      "290 [D loss: 0.825161, acc.: 42.19%] [G loss: 0.904492]\n",
      "291 [D loss: 0.842152, acc.: 46.88%] [G loss: 0.713830]\n",
      "292 [D loss: 0.844464, acc.: 34.38%] [G loss: 1.016867]\n",
      "293 [D loss: 0.842789, acc.: 50.00%] [G loss: 0.850688]\n",
      "294 [D loss: 0.793187, acc.: 48.44%] [G loss: 0.912473]\n",
      "295 [D loss: 0.703839, acc.: 59.38%] [G loss: 0.938225]\n",
      "296 [D loss: 0.757965, acc.: 57.81%] [G loss: 1.075675]\n",
      "297 [D loss: 0.882329, acc.: 37.50%] [G loss: 1.009282]\n",
      "298 [D loss: 0.730610, acc.: 48.44%] [G loss: 0.994001]\n",
      "299 [D loss: 0.869489, acc.: 42.19%] [G loss: 0.902745]\n",
      "300 [D loss: 0.758851, acc.: 50.00%] [G loss: 1.171219]\n",
      "301 [D loss: 0.834852, acc.: 43.75%] [G loss: 1.155959]\n",
      "302 [D loss: 0.859785, acc.: 34.38%] [G loss: 0.920364]\n",
      "303 [D loss: 0.866738, acc.: 46.88%] [G loss: 1.078046]\n",
      "304 [D loss: 0.780605, acc.: 53.12%] [G loss: 1.187156]\n",
      "305 [D loss: 0.784307, acc.: 54.69%] [G loss: 1.030689]\n",
      "306 [D loss: 0.822042, acc.: 37.50%] [G loss: 0.941109]\n",
      "307 [D loss: 0.778324, acc.: 54.69%] [G loss: 0.832884]\n",
      "308 [D loss: 0.840450, acc.: 50.00%] [G loss: 0.969212]\n",
      "309 [D loss: 0.809089, acc.: 48.44%] [G loss: 0.991157]\n",
      "310 [D loss: 0.901700, acc.: 40.62%] [G loss: 0.946711]\n",
      "311 [D loss: 0.836867, acc.: 42.19%] [G loss: 0.890598]\n",
      "312 [D loss: 0.806903, acc.: 53.12%] [G loss: 0.804219]\n",
      "313 [D loss: 0.759400, acc.: 51.56%] [G loss: 0.960849]\n",
      "314 [D loss: 0.873625, acc.: 43.75%] [G loss: 0.847833]\n",
      "315 [D loss: 0.798881, acc.: 51.56%] [G loss: 0.899582]\n",
      "316 [D loss: 0.897841, acc.: 34.38%] [G loss: 0.918892]\n",
      "317 [D loss: 0.723758, acc.: 54.69%] [G loss: 1.011151]\n",
      "318 [D loss: 1.011083, acc.: 40.62%] [G loss: 1.013945]\n",
      "319 [D loss: 0.849342, acc.: 43.75%] [G loss: 1.239778]\n",
      "320 [D loss: 0.803965, acc.: 48.44%] [G loss: 0.989669]\n",
      "321 [D loss: 0.856715, acc.: 43.75%] [G loss: 1.038347]\n",
      "322 [D loss: 0.854796, acc.: 45.31%] [G loss: 0.870811]\n",
      "323 [D loss: 0.957800, acc.: 32.81%] [G loss: 0.925254]\n",
      "324 [D loss: 0.805991, acc.: 45.31%] [G loss: 0.884399]\n",
      "325 [D loss: 0.966236, acc.: 29.69%] [G loss: 0.984973]\n",
      "326 [D loss: 0.682451, acc.: 56.25%] [G loss: 1.052912]\n",
      "327 [D loss: 0.822278, acc.: 46.88%] [G loss: 1.026681]\n",
      "328 [D loss: 0.827444, acc.: 46.88%] [G loss: 1.153428]\n",
      "329 [D loss: 0.755309, acc.: 54.69%] [G loss: 1.053104]\n",
      "330 [D loss: 0.864553, acc.: 45.31%] [G loss: 1.105537]\n",
      "331 [D loss: 0.852859, acc.: 46.88%] [G loss: 0.883674]\n",
      "332 [D loss: 0.920557, acc.: 34.38%] [G loss: 0.993550]\n",
      "333 [D loss: 0.774934, acc.: 48.44%] [G loss: 1.112016]\n",
      "334 [D loss: 0.817983, acc.: 46.88%] [G loss: 0.916745]\n",
      "335 [D loss: 0.818527, acc.: 46.88%] [G loss: 1.016877]\n",
      "336 [D loss: 0.902820, acc.: 46.88%] [G loss: 1.046945]\n",
      "337 [D loss: 0.712613, acc.: 54.69%] [G loss: 0.892720]\n",
      "338 [D loss: 0.870463, acc.: 43.75%] [G loss: 0.890993]\n",
      "339 [D loss: 0.890476, acc.: 46.88%] [G loss: 1.109385]\n",
      "340 [D loss: 0.806040, acc.: 43.75%] [G loss: 1.238895]\n",
      "341 [D loss: 0.778901, acc.: 48.44%] [G loss: 0.985196]\n",
      "342 [D loss: 0.890240, acc.: 40.62%] [G loss: 0.947409]\n",
      "343 [D loss: 0.700242, acc.: 48.44%] [G loss: 0.862394]\n",
      "344 [D loss: 0.756411, acc.: 53.12%] [G loss: 1.039240]\n",
      "345 [D loss: 0.857876, acc.: 43.75%] [G loss: 0.950300]\n",
      "346 [D loss: 0.819594, acc.: 45.31%] [G loss: 0.975198]\n",
      "347 [D loss: 0.868814, acc.: 35.94%] [G loss: 0.963701]\n",
      "348 [D loss: 0.909517, acc.: 37.50%] [G loss: 0.997647]\n",
      "349 [D loss: 0.714077, acc.: 62.50%] [G loss: 1.059912]\n",
      "350 [D loss: 0.756938, acc.: 54.69%] [G loss: 0.872691]\n",
      "351 [D loss: 0.822860, acc.: 42.19%] [G loss: 1.115667]\n",
      "352 [D loss: 0.757806, acc.: 48.44%] [G loss: 1.014544]\n",
      "353 [D loss: 0.740276, acc.: 56.25%] [G loss: 0.844192]\n",
      "354 [D loss: 0.780952, acc.: 48.44%] [G loss: 0.978603]\n",
      "355 [D loss: 0.875570, acc.: 34.38%] [G loss: 1.043908]\n",
      "356 [D loss: 0.915359, acc.: 31.25%] [G loss: 0.736158]\n",
      "357 [D loss: 0.801248, acc.: 46.88%] [G loss: 0.948083]\n",
      "358 [D loss: 0.810937, acc.: 42.19%] [G loss: 1.023326]\n",
      "359 [D loss: 0.851852, acc.: 42.19%] [G loss: 0.739640]\n",
      "360 [D loss: 0.842655, acc.: 45.31%] [G loss: 0.929178]\n",
      "361 [D loss: 0.846213, acc.: 43.75%] [G loss: 0.786692]\n",
      "362 [D loss: 0.737789, acc.: 59.38%] [G loss: 0.889740]\n",
      "363 [D loss: 0.828254, acc.: 43.75%] [G loss: 1.102870]\n",
      "364 [D loss: 0.821126, acc.: 48.44%] [G loss: 1.097025]\n",
      "365 [D loss: 0.782156, acc.: 45.31%] [G loss: 0.964956]\n",
      "366 [D loss: 0.773440, acc.: 54.69%] [G loss: 1.027070]\n",
      "367 [D loss: 0.763617, acc.: 51.56%] [G loss: 0.943089]\n",
      "368 [D loss: 0.806513, acc.: 53.12%] [G loss: 0.773762]\n",
      "369 [D loss: 0.793877, acc.: 43.75%] [G loss: 0.928291]\n",
      "370 [D loss: 0.755061, acc.: 50.00%] [G loss: 0.933281]\n",
      "371 [D loss: 0.753048, acc.: 50.00%] [G loss: 1.004808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "372 [D loss: 0.760131, acc.: 42.19%] [G loss: 1.031789]\n",
      "373 [D loss: 0.762250, acc.: 56.25%] [G loss: 1.088920]\n",
      "374 [D loss: 0.762112, acc.: 46.88%] [G loss: 0.960469]\n",
      "375 [D loss: 0.694089, acc.: 53.12%] [G loss: 0.975161]\n",
      "376 [D loss: 0.919875, acc.: 40.62%] [G loss: 0.924743]\n",
      "377 [D loss: 0.802324, acc.: 48.44%] [G loss: 0.904683]\n",
      "378 [D loss: 0.784866, acc.: 45.31%] [G loss: 0.930586]\n",
      "379 [D loss: 0.815278, acc.: 46.88%] [G loss: 1.028180]\n",
      "380 [D loss: 0.881044, acc.: 43.75%] [G loss: 0.941486]\n",
      "381 [D loss: 0.882279, acc.: 40.62%] [G loss: 1.095813]\n",
      "382 [D loss: 0.700932, acc.: 59.38%] [G loss: 0.908998]\n",
      "383 [D loss: 0.776947, acc.: 45.31%] [G loss: 0.985399]\n",
      "384 [D loss: 0.816980, acc.: 37.50%] [G loss: 0.940297]\n",
      "385 [D loss: 0.772488, acc.: 50.00%] [G loss: 0.890104]\n",
      "386 [D loss: 0.712681, acc.: 53.12%] [G loss: 0.863937]\n",
      "387 [D loss: 0.826732, acc.: 46.88%] [G loss: 0.909253]\n",
      "388 [D loss: 0.910226, acc.: 32.81%] [G loss: 0.843665]\n",
      "389 [D loss: 0.763310, acc.: 50.00%] [G loss: 0.863141]\n",
      "390 [D loss: 0.757947, acc.: 54.69%] [G loss: 0.995741]\n",
      "391 [D loss: 0.832604, acc.: 45.31%] [G loss: 1.105892]\n",
      "392 [D loss: 0.847675, acc.: 45.31%] [G loss: 0.797595]\n",
      "393 [D loss: 0.783452, acc.: 53.12%] [G loss: 0.882528]\n",
      "394 [D loss: 0.723241, acc.: 62.50%] [G loss: 1.023365]\n",
      "395 [D loss: 0.788031, acc.: 48.44%] [G loss: 0.929394]\n",
      "396 [D loss: 0.807696, acc.: 42.19%] [G loss: 0.997207]\n",
      "397 [D loss: 0.743043, acc.: 60.94%] [G loss: 1.005647]\n",
      "398 [D loss: 0.723805, acc.: 57.81%] [G loss: 1.121508]\n",
      "399 [D loss: 0.853934, acc.: 37.50%] [G loss: 0.988724]\n",
      "400 [D loss: 0.946080, acc.: 35.94%] [G loss: 1.107118]\n",
      "401 [D loss: 0.861856, acc.: 43.75%] [G loss: 1.012818]\n",
      "402 [D loss: 0.851551, acc.: 40.62%] [G loss: 1.028262]\n",
      "403 [D loss: 0.861413, acc.: 40.62%] [G loss: 1.055310]\n",
      "404 [D loss: 0.770303, acc.: 51.56%] [G loss: 0.959340]\n",
      "405 [D loss: 0.719897, acc.: 54.69%] [G loss: 1.073328]\n",
      "406 [D loss: 0.805802, acc.: 45.31%] [G loss: 0.902416]\n",
      "407 [D loss: 0.715251, acc.: 60.94%] [G loss: 0.913980]\n",
      "408 [D loss: 0.719441, acc.: 59.38%] [G loss: 0.946474]\n",
      "409 [D loss: 0.807408, acc.: 40.62%] [G loss: 1.042236]\n",
      "410 [D loss: 0.837492, acc.: 37.50%] [G loss: 0.927392]\n",
      "411 [D loss: 0.738640, acc.: 51.56%] [G loss: 0.989921]\n",
      "412 [D loss: 0.711397, acc.: 50.00%] [G loss: 0.955491]\n",
      "413 [D loss: 0.778058, acc.: 51.56%] [G loss: 0.967929]\n",
      "414 [D loss: 0.852602, acc.: 43.75%] [G loss: 1.025621]\n",
      "415 [D loss: 0.788915, acc.: 50.00%] [G loss: 1.079226]\n",
      "416 [D loss: 0.834641, acc.: 42.19%] [G loss: 1.018823]\n",
      "417 [D loss: 0.785881, acc.: 40.62%] [G loss: 1.173727]\n",
      "418 [D loss: 0.671914, acc.: 60.94%] [G loss: 1.028766]\n",
      "419 [D loss: 0.866539, acc.: 46.88%] [G loss: 0.995995]\n",
      "420 [D loss: 0.908236, acc.: 50.00%] [G loss: 0.973222]\n",
      "421 [D loss: 0.803765, acc.: 43.75%] [G loss: 0.855159]\n",
      "422 [D loss: 0.751026, acc.: 51.56%] [G loss: 1.001472]\n",
      "423 [D loss: 0.864603, acc.: 34.38%] [G loss: 0.909463]\n",
      "424 [D loss: 0.870344, acc.: 42.19%] [G loss: 0.847487]\n",
      "425 [D loss: 0.778628, acc.: 50.00%] [G loss: 0.976328]\n",
      "426 [D loss: 0.822627, acc.: 46.88%] [G loss: 1.051281]\n",
      "427 [D loss: 0.714546, acc.: 54.69%] [G loss: 0.862775]\n",
      "428 [D loss: 0.799972, acc.: 51.56%] [G loss: 0.840949]\n",
      "429 [D loss: 0.737769, acc.: 46.88%] [G loss: 0.977471]\n",
      "430 [D loss: 0.843864, acc.: 34.38%] [G loss: 0.900735]\n",
      "431 [D loss: 0.757074, acc.: 50.00%] [G loss: 0.838552]\n",
      "432 [D loss: 0.817736, acc.: 39.06%] [G loss: 0.850358]\n",
      "433 [D loss: 0.702903, acc.: 50.00%] [G loss: 1.000816]\n",
      "434 [D loss: 0.696941, acc.: 51.56%] [G loss: 0.923188]\n",
      "435 [D loss: 0.855252, acc.: 45.31%] [G loss: 0.901746]\n",
      "436 [D loss: 0.821915, acc.: 48.44%] [G loss: 0.882744]\n",
      "437 [D loss: 0.881208, acc.: 39.06%] [G loss: 1.130580]\n",
      "438 [D loss: 0.771314, acc.: 46.88%] [G loss: 0.972939]\n",
      "439 [D loss: 0.749272, acc.: 48.44%] [G loss: 0.933509]\n",
      "440 [D loss: 0.736111, acc.: 56.25%] [G loss: 0.903723]\n",
      "441 [D loss: 0.796110, acc.: 45.31%] [G loss: 1.012941]\n",
      "442 [D loss: 0.881825, acc.: 42.19%] [G loss: 0.817607]\n",
      "443 [D loss: 0.792194, acc.: 57.81%] [G loss: 0.957097]\n",
      "444 [D loss: 0.745744, acc.: 51.56%] [G loss: 0.970758]\n",
      "445 [D loss: 0.731878, acc.: 42.19%] [G loss: 0.921899]\n",
      "446 [D loss: 0.787087, acc.: 45.31%] [G loss: 0.888962]\n",
      "447 [D loss: 0.661582, acc.: 62.50%] [G loss: 1.000412]\n",
      "448 [D loss: 0.822490, acc.: 39.06%] [G loss: 0.973943]\n",
      "449 [D loss: 0.818003, acc.: 45.31%] [G loss: 0.906033]\n",
      "450 [D loss: 0.756505, acc.: 51.56%] [G loss: 1.058363]\n",
      "451 [D loss: 0.773500, acc.: 48.44%] [G loss: 1.077837]\n",
      "452 [D loss: 0.789224, acc.: 51.56%] [G loss: 0.944121]\n",
      "453 [D loss: 0.779976, acc.: 53.12%] [G loss: 0.842399]\n",
      "454 [D loss: 0.782172, acc.: 51.56%] [G loss: 1.102303]\n",
      "455 [D loss: 0.738367, acc.: 56.25%] [G loss: 0.983410]\n",
      "456 [D loss: 0.818563, acc.: 46.88%] [G loss: 0.909298]\n",
      "457 [D loss: 0.773915, acc.: 45.31%] [G loss: 0.929343]\n",
      "458 [D loss: 0.820050, acc.: 46.88%] [G loss: 0.821729]\n",
      "459 [D loss: 0.722007, acc.: 60.94%] [G loss: 0.806890]\n",
      "460 [D loss: 0.678133, acc.: 57.81%] [G loss: 0.904206]\n",
      "461 [D loss: 0.810041, acc.: 42.19%] [G loss: 1.030740]\n",
      "462 [D loss: 0.875067, acc.: 35.94%] [G loss: 0.788570]\n",
      "463 [D loss: 0.807673, acc.: 45.31%] [G loss: 0.856993]\n",
      "464 [D loss: 0.820012, acc.: 40.62%] [G loss: 1.027280]\n",
      "465 [D loss: 0.815657, acc.: 46.88%] [G loss: 1.048290]\n",
      "466 [D loss: 0.826446, acc.: 43.75%] [G loss: 0.964826]\n",
      "467 [D loss: 0.772648, acc.: 51.56%] [G loss: 1.020278]\n",
      "468 [D loss: 0.734308, acc.: 54.69%] [G loss: 1.031252]\n",
      "469 [D loss: 0.939888, acc.: 31.25%] [G loss: 0.866403]\n",
      "470 [D loss: 0.764197, acc.: 56.25%] [G loss: 0.908425]\n",
      "471 [D loss: 0.785286, acc.: 45.31%] [G loss: 1.161967]\n",
      "472 [D loss: 0.757254, acc.: 57.81%] [G loss: 0.919809]\n",
      "473 [D loss: 0.782843, acc.: 48.44%] [G loss: 1.230965]\n",
      "474 [D loss: 0.855878, acc.: 40.62%] [G loss: 1.055232]\n",
      "475 [D loss: 0.830667, acc.: 46.88%] [G loss: 1.015177]\n",
      "476 [D loss: 0.790513, acc.: 34.38%] [G loss: 1.035378]\n",
      "477 [D loss: 0.810450, acc.: 37.50%] [G loss: 0.942257]\n",
      "478 [D loss: 0.758947, acc.: 53.12%] [G loss: 0.835044]\n",
      "479 [D loss: 0.703577, acc.: 60.94%] [G loss: 0.900741]\n",
      "480 [D loss: 0.802470, acc.: 54.69%] [G loss: 0.995963]\n",
      "481 [D loss: 0.796519, acc.: 59.38%] [G loss: 0.994231]\n",
      "482 [D loss: 0.805115, acc.: 50.00%] [G loss: 0.762115]\n",
      "483 [D loss: 0.799373, acc.: 42.19%] [G loss: 0.868437]\n",
      "484 [D loss: 0.882822, acc.: 29.69%] [G loss: 1.126612]\n",
      "485 [D loss: 0.780053, acc.: 50.00%] [G loss: 0.825866]\n",
      "486 [D loss: 0.782451, acc.: 54.69%] [G loss: 0.958172]\n",
      "487 [D loss: 0.828459, acc.: 45.31%] [G loss: 0.829853]\n",
      "488 [D loss: 0.734251, acc.: 48.44%] [G loss: 1.097905]\n",
      "489 [D loss: 0.810618, acc.: 48.44%] [G loss: 0.823648]\n",
      "490 [D loss: 0.749457, acc.: 46.88%] [G loss: 1.030703]\n",
      "491 [D loss: 0.744471, acc.: 54.69%] [G loss: 0.966068]\n",
      "492 [D loss: 0.793356, acc.: 46.88%] [G loss: 0.995190]\n",
      "493 [D loss: 0.783073, acc.: 43.75%] [G loss: 1.017756]\n",
      "494 [D loss: 0.820658, acc.: 43.75%] [G loss: 0.945195]\n",
      "495 [D loss: 0.747880, acc.: 40.62%] [G loss: 0.888105]\n",
      "496 [D loss: 0.790857, acc.: 42.19%] [G loss: 0.917267]\n",
      "497 [D loss: 0.760072, acc.: 48.44%] [G loss: 0.967591]\n",
      "498 [D loss: 0.771826, acc.: 54.69%] [G loss: 0.917940]\n",
      "499 [D loss: 0.834318, acc.: 37.50%] [G loss: 1.055977]\n",
      "500 [D loss: 0.784176, acc.: 48.44%] [G loss: 0.993597]\n",
      "501 [D loss: 0.838598, acc.: 43.75%] [G loss: 0.981588]\n",
      "502 [D loss: 0.670388, acc.: 60.94%] [G loss: 1.077649]\n",
      "503 [D loss: 0.809511, acc.: 48.44%] [G loss: 1.110776]\n",
      "504 [D loss: 0.790030, acc.: 51.56%] [G loss: 0.929539]\n",
      "505 [D loss: 0.727205, acc.: 48.44%] [G loss: 1.033545]\n",
      "506 [D loss: 0.666096, acc.: 59.38%] [G loss: 0.922251]\n",
      "507 [D loss: 0.748459, acc.: 53.12%] [G loss: 0.945754]\n",
      "508 [D loss: 0.804390, acc.: 43.75%] [G loss: 1.009086]\n",
      "509 [D loss: 0.860794, acc.: 37.50%] [G loss: 1.021452]\n",
      "510 [D loss: 0.769165, acc.: 51.56%] [G loss: 0.970174]\n",
      "511 [D loss: 0.833623, acc.: 39.06%] [G loss: 0.936778]\n",
      "512 [D loss: 0.764325, acc.: 46.88%] [G loss: 0.821743]\n",
      "513 [D loss: 0.792679, acc.: 43.75%] [G loss: 0.876735]\n",
      "514 [D loss: 0.740117, acc.: 48.44%] [G loss: 0.996141]\n",
      "515 [D loss: 0.750326, acc.: 53.12%] [G loss: 0.957165]\n",
      "516 [D loss: 0.809036, acc.: 46.88%] [G loss: 0.870333]\n",
      "517 [D loss: 0.787356, acc.: 50.00%] [G loss: 0.956684]\n",
      "518 [D loss: 0.786765, acc.: 40.62%] [G loss: 0.910050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "519 [D loss: 0.756301, acc.: 50.00%] [G loss: 0.877605]\n",
      "520 [D loss: 0.744924, acc.: 54.69%] [G loss: 0.954007]\n",
      "521 [D loss: 0.796599, acc.: 46.88%] [G loss: 1.097318]\n",
      "522 [D loss: 0.901040, acc.: 42.19%] [G loss: 0.841578]\n",
      "523 [D loss: 0.786325, acc.: 53.12%] [G loss: 0.989040]\n",
      "524 [D loss: 0.735413, acc.: 53.12%] [G loss: 0.934489]\n",
      "525 [D loss: 0.686691, acc.: 56.25%] [G loss: 1.091615]\n",
      "526 [D loss: 0.829497, acc.: 37.50%] [G loss: 0.743876]\n",
      "527 [D loss: 0.754539, acc.: 45.31%] [G loss: 1.072405]\n",
      "528 [D loss: 0.686444, acc.: 56.25%] [G loss: 0.924680]\n",
      "529 [D loss: 0.867660, acc.: 42.19%] [G loss: 0.927123]\n",
      "530 [D loss: 0.825425, acc.: 40.62%] [G loss: 0.999630]\n",
      "531 [D loss: 0.818684, acc.: 46.88%] [G loss: 1.063950]\n",
      "532 [D loss: 0.794444, acc.: 43.75%] [G loss: 0.934129]\n",
      "533 [D loss: 0.823258, acc.: 40.62%] [G loss: 0.995812]\n",
      "534 [D loss: 0.797544, acc.: 42.19%] [G loss: 1.004571]\n",
      "535 [D loss: 0.690372, acc.: 65.62%] [G loss: 0.903375]\n",
      "536 [D loss: 0.812494, acc.: 45.31%] [G loss: 0.933998]\n",
      "537 [D loss: 0.792688, acc.: 50.00%] [G loss: 1.067160]\n",
      "538 [D loss: 0.721217, acc.: 53.12%] [G loss: 0.867199]\n",
      "539 [D loss: 0.645449, acc.: 64.06%] [G loss: 0.981472]\n",
      "540 [D loss: 0.683919, acc.: 50.00%] [G loss: 1.043088]\n",
      "541 [D loss: 0.807863, acc.: 40.62%] [G loss: 0.966912]\n",
      "542 [D loss: 0.829184, acc.: 43.75%] [G loss: 1.067524]\n",
      "543 [D loss: 0.679934, acc.: 59.38%] [G loss: 0.932051]\n",
      "544 [D loss: 0.757902, acc.: 59.38%] [G loss: 0.984553]\n",
      "545 [D loss: 0.679020, acc.: 64.06%] [G loss: 0.931420]\n",
      "546 [D loss: 0.747827, acc.: 60.94%] [G loss: 0.925810]\n",
      "547 [D loss: 0.616548, acc.: 60.94%] [G loss: 0.911103]\n",
      "548 [D loss: 0.899021, acc.: 37.50%] [G loss: 0.887244]\n",
      "549 [D loss: 0.710865, acc.: 54.69%] [G loss: 0.953266]\n",
      "550 [D loss: 0.836536, acc.: 46.88%] [G loss: 0.921374]\n",
      "551 [D loss: 0.775701, acc.: 43.75%] [G loss: 0.980755]\n",
      "552 [D loss: 0.756500, acc.: 51.56%] [G loss: 0.951308]\n",
      "553 [D loss: 0.726174, acc.: 53.12%] [G loss: 0.940876]\n",
      "554 [D loss: 0.807423, acc.: 39.06%] [G loss: 1.046769]\n",
      "555 [D loss: 0.710249, acc.: 53.12%] [G loss: 1.068577]\n",
      "556 [D loss: 0.785463, acc.: 50.00%] [G loss: 0.849627]\n",
      "557 [D loss: 0.818156, acc.: 48.44%] [G loss: 0.948916]\n",
      "558 [D loss: 0.732873, acc.: 59.38%] [G loss: 0.783187]\n",
      "559 [D loss: 0.735606, acc.: 56.25%] [G loss: 0.936676]\n",
      "560 [D loss: 0.876735, acc.: 32.81%] [G loss: 0.896692]\n",
      "561 [D loss: 0.752249, acc.: 50.00%] [G loss: 1.057265]\n",
      "562 [D loss: 0.737450, acc.: 51.56%] [G loss: 1.037006]\n",
      "563 [D loss: 0.809979, acc.: 50.00%] [G loss: 0.925993]\n",
      "564 [D loss: 0.743756, acc.: 53.12%] [G loss: 0.939209]\n",
      "565 [D loss: 0.801253, acc.: 40.62%] [G loss: 1.019846]\n",
      "566 [D loss: 0.718535, acc.: 54.69%] [G loss: 0.789018]\n",
      "567 [D loss: 0.759854, acc.: 51.56%] [G loss: 0.881559]\n",
      "568 [D loss: 0.688774, acc.: 48.44%] [G loss: 0.918471]\n",
      "569 [D loss: 0.813823, acc.: 43.75%] [G loss: 0.889873]\n",
      "570 [D loss: 0.722267, acc.: 54.69%] [G loss: 1.011693]\n",
      "571 [D loss: 0.738537, acc.: 50.00%] [G loss: 0.998413]\n",
      "572 [D loss: 0.811223, acc.: 43.75%] [G loss: 1.056823]\n",
      "573 [D loss: 0.708121, acc.: 54.69%] [G loss: 0.877397]\n",
      "574 [D loss: 0.778024, acc.: 51.56%] [G loss: 0.847648]\n",
      "575 [D loss: 0.757702, acc.: 57.81%] [G loss: 0.912069]\n",
      "576 [D loss: 0.745910, acc.: 50.00%] [G loss: 0.985423]\n",
      "577 [D loss: 0.738421, acc.: 51.56%] [G loss: 0.965790]\n",
      "578 [D loss: 0.741434, acc.: 48.44%] [G loss: 0.903037]\n",
      "579 [D loss: 0.667675, acc.: 64.06%] [G loss: 0.919508]\n",
      "580 [D loss: 0.806062, acc.: 42.19%] [G loss: 1.006557]\n",
      "581 [D loss: 0.737872, acc.: 50.00%] [G loss: 1.048717]\n",
      "582 [D loss: 0.716372, acc.: 50.00%] [G loss: 1.015687]\n",
      "583 [D loss: 0.795580, acc.: 43.75%] [G loss: 0.889947]\n",
      "584 [D loss: 0.773748, acc.: 51.56%] [G loss: 0.975282]\n",
      "585 [D loss: 0.800747, acc.: 40.62%] [G loss: 0.957173]\n",
      "586 [D loss: 0.756478, acc.: 50.00%] [G loss: 1.012642]\n",
      "587 [D loss: 0.689197, acc.: 60.94%] [G loss: 0.928332]\n",
      "588 [D loss: 0.738775, acc.: 56.25%] [G loss: 1.094518]\n",
      "589 [D loss: 0.772609, acc.: 50.00%] [G loss: 1.201550]\n",
      "590 [D loss: 0.759055, acc.: 50.00%] [G loss: 1.004198]\n",
      "591 [D loss: 0.737138, acc.: 50.00%] [G loss: 0.918800]\n",
      "592 [D loss: 0.715992, acc.: 56.25%] [G loss: 0.989275]\n",
      "593 [D loss: 0.808798, acc.: 46.88%] [G loss: 1.041060]\n",
      "594 [D loss: 0.753359, acc.: 46.88%] [G loss: 0.931231]\n",
      "595 [D loss: 0.710933, acc.: 54.69%] [G loss: 1.094546]\n",
      "596 [D loss: 0.655886, acc.: 59.38%] [G loss: 0.985174]\n",
      "597 [D loss: 0.836088, acc.: 45.31%] [G loss: 1.038316]\n",
      "598 [D loss: 0.811811, acc.: 40.62%] [G loss: 0.972048]\n",
      "599 [D loss: 0.727494, acc.: 56.25%] [G loss: 1.032452]\n",
      "600 [D loss: 0.779391, acc.: 43.75%] [G loss: 0.915917]\n",
      "601 [D loss: 0.690979, acc.: 59.38%] [G loss: 1.022299]\n",
      "602 [D loss: 0.689644, acc.: 64.06%] [G loss: 0.842903]\n",
      "603 [D loss: 0.848218, acc.: 42.19%] [G loss: 0.912120]\n",
      "604 [D loss: 0.688484, acc.: 64.06%] [G loss: 1.037013]\n",
      "605 [D loss: 0.807454, acc.: 40.62%] [G loss: 0.929926]\n",
      "606 [D loss: 0.805056, acc.: 39.06%] [G loss: 0.848020]\n",
      "607 [D loss: 0.814026, acc.: 48.44%] [G loss: 0.979837]\n",
      "608 [D loss: 0.811826, acc.: 51.56%] [G loss: 0.911108]\n",
      "609 [D loss: 0.727092, acc.: 57.81%] [G loss: 0.781315]\n",
      "610 [D loss: 0.749086, acc.: 51.56%] [G loss: 1.017849]\n",
      "611 [D loss: 0.730685, acc.: 57.81%] [G loss: 0.897753]\n",
      "612 [D loss: 0.739223, acc.: 48.44%] [G loss: 0.902990]\n",
      "613 [D loss: 0.815437, acc.: 42.19%] [G loss: 0.973522]\n",
      "614 [D loss: 0.756721, acc.: 48.44%] [G loss: 0.974544]\n",
      "615 [D loss: 0.857566, acc.: 39.06%] [G loss: 0.935052]\n",
      "616 [D loss: 0.755103, acc.: 50.00%] [G loss: 0.872445]\n",
      "617 [D loss: 0.805570, acc.: 43.75%] [G loss: 0.827649]\n",
      "618 [D loss: 0.695971, acc.: 56.25%] [G loss: 0.920671]\n",
      "619 [D loss: 0.840599, acc.: 40.62%] [G loss: 1.075701]\n",
      "620 [D loss: 0.839694, acc.: 39.06%] [G loss: 0.970318]\n",
      "621 [D loss: 0.834691, acc.: 34.38%] [G loss: 0.879260]\n",
      "622 [D loss: 0.683534, acc.: 56.25%] [G loss: 1.148038]\n",
      "623 [D loss: 0.735034, acc.: 53.12%] [G loss: 0.858895]\n",
      "624 [D loss: 0.810289, acc.: 39.06%] [G loss: 0.917710]\n",
      "625 [D loss: 0.740731, acc.: 37.50%] [G loss: 0.945767]\n",
      "626 [D loss: 0.683864, acc.: 53.12%] [G loss: 1.154642]\n",
      "627 [D loss: 0.721190, acc.: 54.69%] [G loss: 1.074764]\n",
      "628 [D loss: 0.767793, acc.: 45.31%] [G loss: 0.855821]\n",
      "629 [D loss: 0.734133, acc.: 48.44%] [G loss: 0.889635]\n",
      "630 [D loss: 0.763897, acc.: 46.88%] [G loss: 1.001626]\n",
      "631 [D loss: 0.756370, acc.: 42.19%] [G loss: 0.929868]\n",
      "632 [D loss: 0.741764, acc.: 54.69%] [G loss: 0.921224]\n",
      "633 [D loss: 0.791616, acc.: 50.00%] [G loss: 0.896866]\n",
      "634 [D loss: 0.833843, acc.: 37.50%] [G loss: 0.796394]\n",
      "635 [D loss: 0.692006, acc.: 51.56%] [G loss: 0.999128]\n",
      "636 [D loss: 0.688863, acc.: 56.25%] [G loss: 0.917194]\n",
      "637 [D loss: 0.685701, acc.: 59.38%] [G loss: 0.930853]\n",
      "638 [D loss: 0.882655, acc.: 37.50%] [G loss: 0.961595]\n",
      "639 [D loss: 0.739537, acc.: 57.81%] [G loss: 0.876206]\n",
      "640 [D loss: 0.862706, acc.: 34.38%] [G loss: 0.811124]\n",
      "641 [D loss: 0.786758, acc.: 48.44%] [G loss: 1.122811]\n",
      "642 [D loss: 0.748340, acc.: 56.25%] [G loss: 0.913110]\n",
      "643 [D loss: 0.894104, acc.: 35.94%] [G loss: 0.943839]\n",
      "644 [D loss: 0.835761, acc.: 43.75%] [G loss: 0.977237]\n",
      "645 [D loss: 0.773747, acc.: 54.69%] [G loss: 0.831513]\n",
      "646 [D loss: 0.666296, acc.: 62.50%] [G loss: 0.966017]\n",
      "647 [D loss: 0.744271, acc.: 53.12%] [G loss: 0.787857]\n",
      "648 [D loss: 0.748204, acc.: 48.44%] [G loss: 1.052508]\n",
      "649 [D loss: 0.780575, acc.: 48.44%] [G loss: 0.917633]\n",
      "650 [D loss: 0.723984, acc.: 53.12%] [G loss: 1.040840]\n",
      "651 [D loss: 0.718083, acc.: 46.88%] [G loss: 0.986002]\n",
      "652 [D loss: 0.800501, acc.: 43.75%] [G loss: 0.864525]\n",
      "653 [D loss: 0.659835, acc.: 64.06%] [G loss: 1.115815]\n",
      "654 [D loss: 0.679897, acc.: 60.94%] [G loss: 1.067820]\n",
      "655 [D loss: 0.819251, acc.: 45.31%] [G loss: 1.018325]\n",
      "656 [D loss: 0.730248, acc.: 50.00%] [G loss: 1.066977]\n",
      "657 [D loss: 0.716497, acc.: 45.31%] [G loss: 0.996185]\n",
      "658 [D loss: 0.796179, acc.: 42.19%] [G loss: 0.968580]\n",
      "659 [D loss: 0.736697, acc.: 48.44%] [G loss: 0.785817]\n",
      "660 [D loss: 0.810286, acc.: 53.12%] [G loss: 1.101007]\n",
      "661 [D loss: 0.795006, acc.: 48.44%] [G loss: 0.957347]\n",
      "662 [D loss: 0.732826, acc.: 48.44%] [G loss: 1.055438]\n",
      "663 [D loss: 0.755967, acc.: 50.00%] [G loss: 0.906238]\n",
      "664 [D loss: 0.751872, acc.: 45.31%] [G loss: 0.919370]\n",
      "665 [D loss: 0.718072, acc.: 56.25%] [G loss: 0.967562]\n",
      "666 [D loss: 0.787671, acc.: 40.62%] [G loss: 1.098264]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667 [D loss: 0.718055, acc.: 54.69%] [G loss: 1.167697]\n",
      "668 [D loss: 0.704033, acc.: 51.56%] [G loss: 0.963759]\n",
      "669 [D loss: 0.803827, acc.: 48.44%] [G loss: 1.018965]\n",
      "670 [D loss: 0.765557, acc.: 50.00%] [G loss: 0.775333]\n",
      "671 [D loss: 0.702239, acc.: 51.56%] [G loss: 1.121590]\n",
      "672 [D loss: 0.788842, acc.: 48.44%] [G loss: 1.000139]\n",
      "673 [D loss: 0.700563, acc.: 50.00%] [G loss: 0.948287]\n",
      "674 [D loss: 0.799315, acc.: 45.31%] [G loss: 0.958538]\n",
      "675 [D loss: 0.728971, acc.: 46.88%] [G loss: 0.887369]\n",
      "676 [D loss: 0.760038, acc.: 51.56%] [G loss: 0.888198]\n",
      "677 [D loss: 0.690623, acc.: 56.25%] [G loss: 1.021244]\n",
      "678 [D loss: 0.824115, acc.: 48.44%] [G loss: 0.946718]\n",
      "679 [D loss: 0.661356, acc.: 56.25%] [G loss: 1.008405]\n",
      "680 [D loss: 0.753410, acc.: 46.88%] [G loss: 1.037211]\n",
      "681 [D loss: 0.737129, acc.: 53.12%] [G loss: 0.947881]\n",
      "682 [D loss: 0.761025, acc.: 54.69%] [G loss: 0.910312]\n",
      "683 [D loss: 0.807082, acc.: 45.31%] [G loss: 0.788077]\n",
      "684 [D loss: 0.875679, acc.: 32.81%] [G loss: 0.807992]\n",
      "685 [D loss: 0.671698, acc.: 56.25%] [G loss: 0.948736]\n",
      "686 [D loss: 0.669267, acc.: 62.50%] [G loss: 1.056270]\n",
      "687 [D loss: 0.687660, acc.: 62.50%] [G loss: 1.058318]\n",
      "688 [D loss: 0.815894, acc.: 42.19%] [G loss: 1.022058]\n",
      "689 [D loss: 0.703127, acc.: 54.69%] [G loss: 0.962808]\n",
      "690 [D loss: 0.662019, acc.: 57.81%] [G loss: 0.996804]\n",
      "691 [D loss: 0.757121, acc.: 45.31%] [G loss: 0.949618]\n",
      "692 [D loss: 0.739193, acc.: 50.00%] [G loss: 0.921039]\n",
      "693 [D loss: 0.726337, acc.: 57.81%] [G loss: 1.021011]\n",
      "694 [D loss: 0.729776, acc.: 54.69%] [G loss: 0.825444]\n",
      "695 [D loss: 0.744318, acc.: 53.12%] [G loss: 0.970265]\n",
      "696 [D loss: 0.788734, acc.: 53.12%] [G loss: 1.000569]\n",
      "697 [D loss: 0.729166, acc.: 56.25%] [G loss: 0.927825]\n",
      "698 [D loss: 0.840953, acc.: 39.06%] [G loss: 0.961204]\n",
      "699 [D loss: 0.787679, acc.: 45.31%] [G loss: 0.875030]\n",
      "700 [D loss: 0.685659, acc.: 54.69%] [G loss: 0.788531]\n",
      "701 [D loss: 0.788716, acc.: 46.88%] [G loss: 1.053106]\n",
      "702 [D loss: 0.708396, acc.: 51.56%] [G loss: 1.119660]\n",
      "703 [D loss: 0.684307, acc.: 62.50%] [G loss: 0.957909]\n",
      "704 [D loss: 0.734628, acc.: 46.88%] [G loss: 0.879050]\n",
      "705 [D loss: 0.747212, acc.: 53.12%] [G loss: 0.915942]\n",
      "706 [D loss: 0.707434, acc.: 57.81%] [G loss: 0.977000]\n",
      "707 [D loss: 0.692139, acc.: 60.94%] [G loss: 0.971613]\n",
      "708 [D loss: 0.647574, acc.: 60.94%] [G loss: 1.004076]\n",
      "709 [D loss: 0.709375, acc.: 50.00%] [G loss: 0.947672]\n",
      "710 [D loss: 0.797852, acc.: 48.44%] [G loss: 0.874572]\n",
      "711 [D loss: 0.700126, acc.: 48.44%] [G loss: 0.909898]\n",
      "712 [D loss: 0.622781, acc.: 65.62%] [G loss: 0.976968]\n",
      "713 [D loss: 0.751080, acc.: 56.25%] [G loss: 0.926019]\n",
      "714 [D loss: 0.871169, acc.: 35.94%] [G loss: 1.134362]\n",
      "715 [D loss: 0.822287, acc.: 45.31%] [G loss: 0.935138]\n",
      "716 [D loss: 0.728731, acc.: 48.44%] [G loss: 0.835796]\n",
      "717 [D loss: 0.718621, acc.: 59.38%] [G loss: 0.962082]\n",
      "718 [D loss: 0.703011, acc.: 56.25%] [G loss: 0.823109]\n",
      "719 [D loss: 0.706771, acc.: 51.56%] [G loss: 0.932085]\n",
      "720 [D loss: 0.691566, acc.: 59.38%] [G loss: 0.864802]\n",
      "721 [D loss: 0.728660, acc.: 51.56%] [G loss: 0.959449]\n",
      "722 [D loss: 0.710515, acc.: 59.38%] [G loss: 0.946536]\n",
      "723 [D loss: 0.717752, acc.: 54.69%] [G loss: 0.969075]\n",
      "724 [D loss: 0.801874, acc.: 54.69%] [G loss: 0.913274]\n",
      "725 [D loss: 0.878066, acc.: 43.75%] [G loss: 1.000690]\n",
      "726 [D loss: 0.685208, acc.: 57.81%] [G loss: 0.962069]\n",
      "727 [D loss: 0.732247, acc.: 56.25%] [G loss: 0.991176]\n",
      "728 [D loss: 0.775476, acc.: 43.75%] [G loss: 0.935504]\n",
      "729 [D loss: 0.751477, acc.: 56.25%] [G loss: 0.887854]\n",
      "730 [D loss: 0.743097, acc.: 51.56%] [G loss: 0.939063]\n",
      "731 [D loss: 0.701049, acc.: 56.25%] [G loss: 1.037063]\n",
      "732 [D loss: 0.740599, acc.: 50.00%] [G loss: 0.769236]\n",
      "733 [D loss: 0.745783, acc.: 45.31%] [G loss: 1.021592]\n",
      "734 [D loss: 0.763094, acc.: 45.31%] [G loss: 1.080293]\n",
      "735 [D loss: 0.829589, acc.: 45.31%] [G loss: 0.892553]\n",
      "736 [D loss: 0.852083, acc.: 46.88%] [G loss: 1.057879]\n",
      "737 [D loss: 0.721934, acc.: 45.31%] [G loss: 0.977095]\n",
      "738 [D loss: 0.744978, acc.: 46.88%] [G loss: 1.151386]\n",
      "739 [D loss: 0.712000, acc.: 53.12%] [G loss: 0.930330]\n",
      "740 [D loss: 0.732026, acc.: 54.69%] [G loss: 0.760751]\n",
      "741 [D loss: 0.770764, acc.: 45.31%] [G loss: 0.989033]\n",
      "742 [D loss: 0.810790, acc.: 50.00%] [G loss: 0.925941]\n",
      "743 [D loss: 0.711536, acc.: 48.44%] [G loss: 0.903131]\n",
      "744 [D loss: 0.797164, acc.: 39.06%] [G loss: 0.887240]\n",
      "745 [D loss: 0.673471, acc.: 51.56%] [G loss: 0.961912]\n",
      "746 [D loss: 0.739789, acc.: 45.31%] [G loss: 0.906458]\n",
      "747 [D loss: 0.766329, acc.: 51.56%] [G loss: 1.004676]\n",
      "748 [D loss: 0.771790, acc.: 51.56%] [G loss: 0.926249]\n",
      "749 [D loss: 0.721900, acc.: 53.12%] [G loss: 0.988478]\n",
      "750 [D loss: 0.803966, acc.: 45.31%] [G loss: 0.959386]\n",
      "751 [D loss: 0.743509, acc.: 57.81%] [G loss: 1.056970]\n",
      "752 [D loss: 0.732291, acc.: 51.56%] [G loss: 1.031772]\n",
      "753 [D loss: 0.743292, acc.: 45.31%] [G loss: 0.931101]\n",
      "754 [D loss: 0.810717, acc.: 46.88%] [G loss: 0.846696]\n",
      "755 [D loss: 0.757685, acc.: 51.56%] [G loss: 1.018178]\n",
      "756 [D loss: 0.776273, acc.: 42.19%] [G loss: 0.949568]\n",
      "757 [D loss: 0.689517, acc.: 62.50%] [G loss: 0.824526]\n",
      "758 [D loss: 0.858125, acc.: 35.94%] [G loss: 0.956535]\n",
      "759 [D loss: 0.731687, acc.: 48.44%] [G loss: 0.906875]\n",
      "760 [D loss: 0.783642, acc.: 40.62%] [G loss: 1.020456]\n",
      "761 [D loss: 0.728155, acc.: 51.56%] [G loss: 0.971316]\n",
      "762 [D loss: 0.623393, acc.: 64.06%] [G loss: 0.871326]\n",
      "763 [D loss: 0.728425, acc.: 56.25%] [G loss: 1.008196]\n",
      "764 [D loss: 0.779735, acc.: 51.56%] [G loss: 1.060517]\n",
      "765 [D loss: 0.705874, acc.: 51.56%] [G loss: 0.959326]\n",
      "766 [D loss: 0.708073, acc.: 59.38%] [G loss: 1.160701]\n",
      "767 [D loss: 0.804338, acc.: 46.88%] [G loss: 1.022695]\n",
      "768 [D loss: 0.770692, acc.: 45.31%] [G loss: 0.959596]\n",
      "769 [D loss: 0.807525, acc.: 40.62%] [G loss: 1.030009]\n",
      "770 [D loss: 0.656543, acc.: 64.06%] [G loss: 0.942937]\n",
      "771 [D loss: 0.702464, acc.: 64.06%] [G loss: 1.028019]\n",
      "772 [D loss: 0.672922, acc.: 53.12%] [G loss: 0.961391]\n",
      "773 [D loss: 0.814229, acc.: 42.19%] [G loss: 1.061241]\n",
      "774 [D loss: 0.816844, acc.: 39.06%] [G loss: 0.845481]\n",
      "775 [D loss: 0.823659, acc.: 39.06%] [G loss: 0.939476]\n",
      "776 [D loss: 0.734664, acc.: 53.12%] [G loss: 0.848402]\n",
      "777 [D loss: 0.674040, acc.: 56.25%] [G loss: 0.962143]\n",
      "778 [D loss: 0.719584, acc.: 56.25%] [G loss: 0.924594]\n",
      "779 [D loss: 0.692108, acc.: 54.69%] [G loss: 0.866248]\n",
      "780 [D loss: 0.746042, acc.: 46.88%] [G loss: 1.027236]\n",
      "781 [D loss: 0.722433, acc.: 54.69%] [G loss: 0.989051]\n",
      "782 [D loss: 0.772606, acc.: 45.31%] [G loss: 0.875433]\n",
      "783 [D loss: 0.742146, acc.: 51.56%] [G loss: 0.862620]\n",
      "784 [D loss: 0.770365, acc.: 45.31%] [G loss: 1.054703]\n",
      "785 [D loss: 0.712004, acc.: 62.50%] [G loss: 0.976635]\n",
      "786 [D loss: 0.725919, acc.: 57.81%] [G loss: 1.097087]\n",
      "787 [D loss: 0.748622, acc.: 48.44%] [G loss: 0.988617]\n",
      "788 [D loss: 0.828123, acc.: 46.88%] [G loss: 0.945171]\n",
      "789 [D loss: 0.727821, acc.: 48.44%] [G loss: 1.107547]\n",
      "790 [D loss: 0.748714, acc.: 56.25%] [G loss: 1.024326]\n",
      "791 [D loss: 0.864660, acc.: 39.06%] [G loss: 0.827583]\n",
      "792 [D loss: 0.797576, acc.: 50.00%] [G loss: 0.891105]\n",
      "793 [D loss: 0.724760, acc.: 60.94%] [G loss: 0.975714]\n",
      "794 [D loss: 0.736307, acc.: 48.44%] [G loss: 1.023582]\n",
      "795 [D loss: 0.693648, acc.: 56.25%] [G loss: 1.136138]\n",
      "796 [D loss: 0.774604, acc.: 51.56%] [G loss: 0.933264]\n",
      "797 [D loss: 0.765047, acc.: 48.44%] [G loss: 0.847379]\n",
      "798 [D loss: 0.763112, acc.: 51.56%] [G loss: 0.815461]\n",
      "799 [D loss: 0.656088, acc.: 64.06%] [G loss: 0.929308]\n",
      "800 [D loss: 0.687809, acc.: 64.06%] [G loss: 0.953036]\n",
      "801 [D loss: 0.818336, acc.: 46.88%] [G loss: 0.984716]\n",
      "802 [D loss: 0.720852, acc.: 56.25%] [G loss: 0.938771]\n",
      "803 [D loss: 0.815417, acc.: 46.88%] [G loss: 0.921246]\n",
      "804 [D loss: 0.616268, acc.: 67.19%] [G loss: 1.096595]\n",
      "805 [D loss: 0.706896, acc.: 50.00%] [G loss: 0.816454]\n",
      "806 [D loss: 0.725813, acc.: 45.31%] [G loss: 0.994738]\n",
      "807 [D loss: 0.738304, acc.: 59.38%] [G loss: 0.834870]\n",
      "808 [D loss: 0.822316, acc.: 39.06%] [G loss: 0.863482]\n",
      "809 [D loss: 0.704744, acc.: 60.94%] [G loss: 0.986172]\n",
      "810 [D loss: 0.795497, acc.: 40.62%] [G loss: 0.919252]\n",
      "811 [D loss: 0.737208, acc.: 51.56%] [G loss: 0.955943]\n",
      "812 [D loss: 0.824087, acc.: 37.50%] [G loss: 1.064865]\n",
      "813 [D loss: 0.774910, acc.: 54.69%] [G loss: 0.774799]\n",
      "814 [D loss: 0.692618, acc.: 54.69%] [G loss: 0.894961]\n",
      "815 [D loss: 0.763921, acc.: 46.88%] [G loss: 0.860504]\n",
      "816 [D loss: 0.728531, acc.: 51.56%] [G loss: 0.833729]\n",
      "817 [D loss: 0.795583, acc.: 46.88%] [G loss: 0.805979]\n",
      "818 [D loss: 0.721865, acc.: 48.44%] [G loss: 0.995442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "819 [D loss: 0.641937, acc.: 67.19%] [G loss: 0.903646]\n",
      "820 [D loss: 0.660612, acc.: 59.38%] [G loss: 1.136652]\n",
      "821 [D loss: 0.834312, acc.: 48.44%] [G loss: 0.906367]\n",
      "822 [D loss: 0.727652, acc.: 56.25%] [G loss: 0.949559]\n",
      "823 [D loss: 0.709825, acc.: 54.69%] [G loss: 0.963095]\n",
      "824 [D loss: 0.842757, acc.: 43.75%] [G loss: 0.852526]\n",
      "825 [D loss: 0.702861, acc.: 60.94%] [G loss: 1.088045]\n",
      "826 [D loss: 0.727600, acc.: 56.25%] [G loss: 0.934490]\n",
      "827 [D loss: 0.736174, acc.: 53.12%] [G loss: 0.975659]\n",
      "828 [D loss: 0.737452, acc.: 54.69%] [G loss: 0.949612]\n",
      "829 [D loss: 0.760531, acc.: 48.44%] [G loss: 0.915557]\n",
      "830 [D loss: 0.765722, acc.: 50.00%] [G loss: 0.874049]\n",
      "831 [D loss: 0.668007, acc.: 65.62%] [G loss: 1.177634]\n",
      "832 [D loss: 0.630142, acc.: 57.81%] [G loss: 0.984288]\n",
      "833 [D loss: 0.652585, acc.: 59.38%] [G loss: 0.973755]\n",
      "834 [D loss: 0.697317, acc.: 50.00%] [G loss: 0.964671]\n",
      "835 [D loss: 0.762977, acc.: 43.75%] [G loss: 0.967145]\n",
      "836 [D loss: 0.817258, acc.: 39.06%] [G loss: 1.018272]\n",
      "837 [D loss: 0.669125, acc.: 59.38%] [G loss: 0.969799]\n",
      "838 [D loss: 0.683859, acc.: 53.12%] [G loss: 0.908086]\n",
      "839 [D loss: 0.617856, acc.: 65.62%] [G loss: 0.953683]\n",
      "840 [D loss: 0.641720, acc.: 64.06%] [G loss: 1.012441]\n",
      "841 [D loss: 0.698532, acc.: 56.25%] [G loss: 1.090013]\n",
      "842 [D loss: 0.812541, acc.: 37.50%] [G loss: 1.056809]\n",
      "843 [D loss: 0.710382, acc.: 53.12%] [G loss: 1.010446]\n",
      "844 [D loss: 0.692943, acc.: 56.25%] [G loss: 1.014332]\n",
      "845 [D loss: 0.676667, acc.: 60.94%] [G loss: 0.897097]\n",
      "846 [D loss: 0.795229, acc.: 48.44%] [G loss: 0.860563]\n",
      "847 [D loss: 0.684002, acc.: 59.38%] [G loss: 0.891889]\n",
      "848 [D loss: 0.663410, acc.: 57.81%] [G loss: 0.904475]\n",
      "849 [D loss: 0.616353, acc.: 64.06%] [G loss: 0.777366]\n",
      "850 [D loss: 0.735284, acc.: 53.12%] [G loss: 0.985005]\n",
      "851 [D loss: 0.759607, acc.: 45.31%] [G loss: 0.906592]\n",
      "852 [D loss: 0.683195, acc.: 62.50%] [G loss: 1.014635]\n",
      "853 [D loss: 0.674946, acc.: 60.94%] [G loss: 1.026218]\n",
      "854 [D loss: 0.675488, acc.: 59.38%] [G loss: 1.152710]\n",
      "855 [D loss: 0.700253, acc.: 53.12%] [G loss: 0.945997]\n",
      "856 [D loss: 0.693134, acc.: 59.38%] [G loss: 0.995624]\n",
      "857 [D loss: 0.721549, acc.: 48.44%] [G loss: 1.066202]\n",
      "858 [D loss: 0.708929, acc.: 53.12%] [G loss: 1.013724]\n",
      "859 [D loss: 0.748733, acc.: 51.56%] [G loss: 0.890964]\n",
      "860 [D loss: 0.818363, acc.: 37.50%] [G loss: 0.819139]\n",
      "861 [D loss: 0.659975, acc.: 64.06%] [G loss: 0.891984]\n",
      "862 [D loss: 0.711547, acc.: 60.94%] [G loss: 0.936049]\n",
      "863 [D loss: 0.724638, acc.: 51.56%] [G loss: 0.903968]\n",
      "864 [D loss: 0.815500, acc.: 43.75%] [G loss: 0.986527]\n",
      "865 [D loss: 0.631083, acc.: 60.94%] [G loss: 1.055975]\n",
      "866 [D loss: 0.806879, acc.: 46.88%] [G loss: 0.857742]\n",
      "867 [D loss: 0.839225, acc.: 45.31%] [G loss: 0.785174]\n",
      "868 [D loss: 0.745673, acc.: 56.25%] [G loss: 0.925882]\n",
      "869 [D loss: 0.711026, acc.: 50.00%] [G loss: 0.918439]\n",
      "870 [D loss: 0.757051, acc.: 54.69%] [G loss: 0.974878]\n",
      "871 [D loss: 0.725302, acc.: 51.56%] [G loss: 1.042809]\n",
      "872 [D loss: 0.707174, acc.: 54.69%] [G loss: 0.993862]\n",
      "873 [D loss: 0.697051, acc.: 57.81%] [G loss: 1.120733]\n",
      "874 [D loss: 0.689175, acc.: 56.25%] [G loss: 1.119316]\n",
      "875 [D loss: 0.714552, acc.: 50.00%] [G loss: 0.933028]\n",
      "876 [D loss: 0.596712, acc.: 68.75%] [G loss: 0.938182]\n",
      "877 [D loss: 0.724981, acc.: 50.00%] [G loss: 0.912689]\n",
      "878 [D loss: 0.744692, acc.: 46.88%] [G loss: 0.994617]\n",
      "879 [D loss: 0.719525, acc.: 50.00%] [G loss: 0.882192]\n",
      "880 [D loss: 0.649061, acc.: 57.81%] [G loss: 1.044139]\n",
      "881 [D loss: 0.748525, acc.: 51.56%] [G loss: 1.142073]\n",
      "882 [D loss: 0.794010, acc.: 51.56%] [G loss: 0.985867]\n",
      "883 [D loss: 0.729186, acc.: 50.00%] [G loss: 0.981389]\n",
      "884 [D loss: 0.812014, acc.: 40.62%] [G loss: 0.957090]\n",
      "885 [D loss: 0.702527, acc.: 57.81%] [G loss: 0.891538]\n",
      "886 [D loss: 0.716569, acc.: 51.56%] [G loss: 0.915079]\n",
      "887 [D loss: 0.690034, acc.: 57.81%] [G loss: 0.821678]\n",
      "888 [D loss: 0.729406, acc.: 57.81%] [G loss: 1.006094]\n",
      "889 [D loss: 0.786377, acc.: 53.12%] [G loss: 0.856350]\n",
      "890 [D loss: 0.696106, acc.: 51.56%] [G loss: 1.099204]\n",
      "891 [D loss: 0.777014, acc.: 46.88%] [G loss: 1.064986]\n",
      "892 [D loss: 0.721352, acc.: 46.88%] [G loss: 0.849963]\n",
      "893 [D loss: 0.810365, acc.: 45.31%] [G loss: 0.902359]\n",
      "894 [D loss: 0.707580, acc.: 53.12%] [G loss: 0.927577]\n",
      "895 [D loss: 0.702191, acc.: 54.69%] [G loss: 0.869264]\n",
      "896 [D loss: 0.700264, acc.: 56.25%] [G loss: 0.986550]\n",
      "897 [D loss: 0.679943, acc.: 53.12%] [G loss: 1.032743]\n",
      "898 [D loss: 0.720370, acc.: 57.81%] [G loss: 1.116221]\n",
      "899 [D loss: 0.755680, acc.: 50.00%] [G loss: 1.145606]\n",
      "900 [D loss: 0.689776, acc.: 60.94%] [G loss: 0.939451]\n",
      "901 [D loss: 0.712106, acc.: 57.81%] [G loss: 0.928016]\n",
      "902 [D loss: 0.722030, acc.: 50.00%] [G loss: 1.041763]\n",
      "903 [D loss: 0.711401, acc.: 56.25%] [G loss: 0.826356]\n",
      "904 [D loss: 0.672493, acc.: 62.50%] [G loss: 0.957271]\n",
      "905 [D loss: 0.718884, acc.: 48.44%] [G loss: 0.965758]\n",
      "906 [D loss: 0.647609, acc.: 65.62%] [G loss: 0.994321]\n",
      "907 [D loss: 0.699963, acc.: 56.25%] [G loss: 1.040923]\n",
      "908 [D loss: 0.774833, acc.: 40.62%] [G loss: 0.932339]\n",
      "909 [D loss: 0.711665, acc.: 51.56%] [G loss: 1.054195]\n",
      "910 [D loss: 0.817528, acc.: 40.62%] [G loss: 0.861803]\n",
      "911 [D loss: 0.766849, acc.: 46.88%] [G loss: 0.943555]\n",
      "912 [D loss: 0.751177, acc.: 48.44%] [G loss: 0.910627]\n",
      "913 [D loss: 0.690053, acc.: 57.81%] [G loss: 0.828788]\n",
      "914 [D loss: 0.745361, acc.: 40.62%] [G loss: 0.884512]\n",
      "915 [D loss: 0.667158, acc.: 56.25%] [G loss: 0.950416]\n",
      "916 [D loss: 0.678347, acc.: 59.38%] [G loss: 0.955363]\n",
      "917 [D loss: 0.695860, acc.: 57.81%] [G loss: 0.888206]\n",
      "918 [D loss: 0.714557, acc.: 50.00%] [G loss: 1.001336]\n",
      "919 [D loss: 0.680548, acc.: 60.94%] [G loss: 0.999524]\n",
      "920 [D loss: 0.746139, acc.: 45.31%] [G loss: 0.972469]\n",
      "921 [D loss: 0.775667, acc.: 46.88%] [G loss: 1.013440]\n",
      "922 [D loss: 0.890784, acc.: 39.06%] [G loss: 0.989094]\n",
      "923 [D loss: 0.763511, acc.: 50.00%] [G loss: 0.845640]\n",
      "924 [D loss: 0.752479, acc.: 50.00%] [G loss: 1.016392]\n",
      "925 [D loss: 0.778805, acc.: 42.19%] [G loss: 0.951633]\n",
      "926 [D loss: 0.722003, acc.: 51.56%] [G loss: 0.924212]\n",
      "927 [D loss: 0.738986, acc.: 59.38%] [G loss: 1.013438]\n",
      "928 [D loss: 0.740646, acc.: 53.12%] [G loss: 1.015016]\n",
      "929 [D loss: 0.695450, acc.: 57.81%] [G loss: 0.877781]\n",
      "930 [D loss: 0.772567, acc.: 46.88%] [G loss: 0.863163]\n",
      "931 [D loss: 0.744854, acc.: 51.56%] [G loss: 0.740170]\n",
      "932 [D loss: 0.695007, acc.: 48.44%] [G loss: 0.967525]\n",
      "933 [D loss: 0.763406, acc.: 48.44%] [G loss: 0.988549]\n",
      "934 [D loss: 0.790693, acc.: 42.19%] [G loss: 0.920001]\n",
      "935 [D loss: 0.724017, acc.: 54.69%] [G loss: 1.092801]\n",
      "936 [D loss: 0.708453, acc.: 54.69%] [G loss: 0.832979]\n",
      "937 [D loss: 0.652781, acc.: 56.25%] [G loss: 1.036443]\n",
      "938 [D loss: 0.622761, acc.: 67.19%] [G loss: 1.006152]\n",
      "939 [D loss: 0.752561, acc.: 45.31%] [G loss: 1.028919]\n",
      "940 [D loss: 0.768160, acc.: 53.12%] [G loss: 0.956273]\n",
      "941 [D loss: 0.710131, acc.: 56.25%] [G loss: 1.041156]\n",
      "942 [D loss: 0.587962, acc.: 71.88%] [G loss: 1.159217]\n",
      "943 [D loss: 0.781627, acc.: 53.12%] [G loss: 0.968813]\n",
      "944 [D loss: 0.811305, acc.: 40.62%] [G loss: 0.950902]\n",
      "945 [D loss: 0.770965, acc.: 50.00%] [G loss: 0.961402]\n",
      "946 [D loss: 0.779106, acc.: 45.31%] [G loss: 0.852890]\n",
      "947 [D loss: 0.752548, acc.: 48.44%] [G loss: 0.865009]\n",
      "948 [D loss: 0.616826, acc.: 59.38%] [G loss: 1.027776]\n",
      "949 [D loss: 0.706052, acc.: 53.12%] [G loss: 0.940289]\n",
      "950 [D loss: 0.696944, acc.: 57.81%] [G loss: 1.052894]\n",
      "951 [D loss: 0.690093, acc.: 50.00%] [G loss: 1.053461]\n",
      "952 [D loss: 0.705170, acc.: 54.69%] [G loss: 0.981156]\n",
      "953 [D loss: 0.856052, acc.: 39.06%] [G loss: 1.110585]\n",
      "954 [D loss: 0.639268, acc.: 64.06%] [G loss: 0.953511]\n",
      "955 [D loss: 0.633362, acc.: 64.06%] [G loss: 0.963750]\n",
      "956 [D loss: 0.739346, acc.: 48.44%] [G loss: 0.839211]\n",
      "957 [D loss: 0.743768, acc.: 57.81%] [G loss: 0.840982]\n",
      "958 [D loss: 0.702705, acc.: 54.69%] [G loss: 1.017578]\n",
      "959 [D loss: 0.708615, acc.: 53.12%] [G loss: 1.008983]\n",
      "960 [D loss: 0.646870, acc.: 59.38%] [G loss: 0.998791]\n",
      "961 [D loss: 0.720122, acc.: 50.00%] [G loss: 0.815983]\n",
      "962 [D loss: 0.777961, acc.: 42.19%] [G loss: 0.875956]\n",
      "963 [D loss: 0.672652, acc.: 60.94%] [G loss: 0.942107]\n",
      "964 [D loss: 0.686320, acc.: 54.69%] [G loss: 1.148678]\n",
      "965 [D loss: 0.779481, acc.: 43.75%] [G loss: 0.831044]\n",
      "966 [D loss: 0.679801, acc.: 57.81%] [G loss: 0.981321]\n",
      "967 [D loss: 0.759825, acc.: 56.25%] [G loss: 0.925499]\n",
      "968 [D loss: 0.700589, acc.: 57.81%] [G loss: 1.011037]\n",
      "969 [D loss: 0.708539, acc.: 56.25%] [G loss: 0.976440]\n",
      "970 [D loss: 0.789632, acc.: 57.81%] [G loss: 1.017766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "971 [D loss: 0.855766, acc.: 53.12%] [G loss: 1.148401]\n",
      "972 [D loss: 0.714060, acc.: 54.69%] [G loss: 0.891527]\n",
      "973 [D loss: 0.756385, acc.: 53.12%] [G loss: 0.889993]\n",
      "974 [D loss: 0.670347, acc.: 54.69%] [G loss: 0.971903]\n",
      "975 [D loss: 0.661932, acc.: 59.38%] [G loss: 0.806111]\n",
      "976 [D loss: 0.711199, acc.: 51.56%] [G loss: 1.013825]\n",
      "977 [D loss: 0.715567, acc.: 56.25%] [G loss: 0.929060]\n",
      "978 [D loss: 0.776022, acc.: 48.44%] [G loss: 0.860867]\n",
      "979 [D loss: 0.759736, acc.: 42.19%] [G loss: 0.893738]\n",
      "980 [D loss: 0.646381, acc.: 54.69%] [G loss: 0.980956]\n",
      "981 [D loss: 0.715525, acc.: 60.94%] [G loss: 1.045639]\n",
      "982 [D loss: 0.754872, acc.: 51.56%] [G loss: 1.081443]\n",
      "983 [D loss: 0.797078, acc.: 40.62%] [G loss: 0.842014]\n",
      "984 [D loss: 0.790355, acc.: 50.00%] [G loss: 0.925009]\n",
      "985 [D loss: 0.773299, acc.: 48.44%] [G loss: 1.043961]\n",
      "986 [D loss: 0.686424, acc.: 57.81%] [G loss: 0.997265]\n",
      "987 [D loss: 0.717744, acc.: 57.81%] [G loss: 0.930494]\n",
      "988 [D loss: 0.749049, acc.: 50.00%] [G loss: 0.910485]\n",
      "989 [D loss: 0.700200, acc.: 54.69%] [G loss: 1.045678]\n",
      "990 [D loss: 0.683583, acc.: 56.25%] [G loss: 0.906096]\n",
      "991 [D loss: 0.702215, acc.: 53.12%] [G loss: 0.965013]\n",
      "992 [D loss: 0.743043, acc.: 57.81%] [G loss: 0.956924]\n",
      "993 [D loss: 0.813193, acc.: 37.50%] [G loss: 0.922732]\n",
      "994 [D loss: 0.724805, acc.: 54.69%] [G loss: 0.843344]\n",
      "995 [D loss: 0.677591, acc.: 60.94%] [G loss: 1.005257]\n",
      "996 [D loss: 0.665061, acc.: 59.38%] [G loss: 0.952836]\n",
      "997 [D loss: 0.654475, acc.: 67.19%] [G loss: 1.166541]\n",
      "998 [D loss: 0.697327, acc.: 53.12%] [G loss: 0.961435]\n",
      "999 [D loss: 0.773407, acc.: 43.75%] [G loss: 0.797774]\n",
      "1000 [D loss: 0.757392, acc.: 48.44%] [G loss: 0.964040]\n",
      "1001 [D loss: 0.694986, acc.: 56.25%] [G loss: 0.988389]\n",
      "1002 [D loss: 0.678684, acc.: 64.06%] [G loss: 0.889135]\n",
      "1003 [D loss: 0.725019, acc.: 53.12%] [G loss: 1.045127]\n",
      "1004 [D loss: 0.708702, acc.: 53.12%] [G loss: 1.037437]\n",
      "1005 [D loss: 0.748410, acc.: 53.12%] [G loss: 0.999801]\n",
      "1006 [D loss: 0.713097, acc.: 53.12%] [G loss: 1.098910]\n",
      "1007 [D loss: 0.714271, acc.: 56.25%] [G loss: 1.057362]\n",
      "1008 [D loss: 0.734262, acc.: 59.38%] [G loss: 1.080998]\n",
      "1009 [D loss: 0.756596, acc.: 51.56%] [G loss: 1.020203]\n",
      "1010 [D loss: 0.622839, acc.: 65.62%] [G loss: 0.917039]\n",
      "1011 [D loss: 0.779976, acc.: 48.44%] [G loss: 1.056910]\n",
      "1012 [D loss: 0.655580, acc.: 56.25%] [G loss: 0.950724]\n",
      "1013 [D loss: 0.756526, acc.: 40.62%] [G loss: 1.011478]\n",
      "1014 [D loss: 0.721161, acc.: 54.69%] [G loss: 0.977375]\n",
      "1015 [D loss: 0.771726, acc.: 46.88%] [G loss: 1.001832]\n",
      "1016 [D loss: 0.711175, acc.: 54.69%] [G loss: 0.852554]\n",
      "1017 [D loss: 0.853705, acc.: 39.06%] [G loss: 0.907312]\n",
      "1018 [D loss: 0.686769, acc.: 51.56%] [G loss: 0.885210]\n",
      "1019 [D loss: 0.710217, acc.: 54.69%] [G loss: 0.953247]\n",
      "1020 [D loss: 0.807581, acc.: 40.62%] [G loss: 0.997810]\n",
      "1021 [D loss: 0.780118, acc.: 56.25%] [G loss: 0.926762]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-49c962d50a65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0mdcgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     \u001b[0mdcgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-49c962d50a65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size, save_interval)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# Train the generator (wants discriminator to mistake images as real)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;31m# Plot the progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 100\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates imgs\n",
    "        z = Input(shape=(self.latent_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 7 * 7, activation=\"relu\", input_dim=self.latent_dim))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        # Load the dataset\n",
    "        (X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = X_train / 127.5 - 1.\n",
    "        X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a batch of new images\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/mnist_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    dcgan.train(epochs=4000, batch_size=32, save_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
